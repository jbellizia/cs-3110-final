{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec433a0c-6409-435f-b5b0-302a51b2f2bb",
   "metadata": {},
   "source": [
    "#### Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc63e2c-204d-4dd6-8718-f45299497086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "# modified gaussian_mech_zCDP_vec to do the gaussian mech on a value with rho (zCDP) instead of alpha, epsilon_bar (RDP) \n",
    "def gaussian_mech_zCDP(v, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return v + np.random.normal(loc=0, scale=sigma)\n",
    "    \n",
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "    \n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "# function written for homework 9, approximates rho based on input epsilon and delta\n",
    "def approximate_rho(epsilon, delta):\n",
    "    # threshold is how close we want to be\n",
    "    threshold = 0.01\n",
    "    # pick an arbitrary original rho\n",
    "    rho = 1\n",
    "    # loop until we get under the threshold\n",
    "    while (True):\n",
    "        # compute epsilon based on our estimated rho and the input values\n",
    "        computed_epsilon = rho + 2 * np.sqrt(rho * np.log(1 / delta))\n",
    "        # compute the difference between them and take the absval to get the actual distance from goal\n",
    "        epsilon_difference = np.abs(computed_epsilon - epsilon)\n",
    "        # if our difference is under the threshold, return rho, we found it\n",
    "        if epsilon_difference < threshold:\n",
    "            return rho\n",
    "        # if our computed epsilon is less than the desired, increase rho by 10%\n",
    "        if (epsilon > computed_epsilon):\n",
    "            rho *= 1.1\n",
    "        # if our computed epsilon is greater than the desired, decrease rho by 10%\n",
    "        elif (epsilon < computed_epsilon):\n",
    "            rho *= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17833ca1-8da5-44ea-b1f4-b659acf8bc88",
   "metadata": {},
   "source": [
    "#### Import the frisbee dataset, drop sensitive and irrelevant columns, one-hot encode categorical columns, and make a label column for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e4795c8-31c2-44c6-9e3e-8bdb7cd070f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the frisbee dataset\n",
    "frisbee = pd.read_csv('https://raw.githubusercontent.com/jbellizia/cs-3110-final/refs/heads/main/ultimate_college_championship.csv')\n",
    "\n",
    "# discard player names and team names, as well as plus minus per game (we use plus minus kind of in the gradient descent)\n",
    "frisbee = frisbee.drop(['player', 'team_name', 'pls_mns_per_game'], axis = 1)\n",
    "\n",
    "# one hot encode the categorical columns \n",
    "frisbee = pd.get_dummies(frisbee, columns=['level', 'gender', 'division'], drop_first = True).astype(int)\n",
    "\n",
    "# add a categorical 'is_positive' column that we will be testing on\n",
    "# -1 if plus_minus is negative\n",
    "# 1 if plus_minus is positive\n",
    "# this way, we can use classification to predict a player's value on a team based on their stats\n",
    "# this evaluates if a players plus minus is positive and stores the truth value as an integer (-1 or 1)\n",
    "# at first, i used 0 and 1, but i think that wasn't strong enough for the model\n",
    "frisbee['is_positive'] = (frisbee['plus_minus'] > 0).astype(int)\n",
    "frisbee['is_positive'] = frisbee['is_positive'] * 2 - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d0015-ac6e-4ea9-b778-2923beb74647",
   "metadata": {},
   "source": [
    "#### Randomize frisbee for testing and training, then split into X (features) and y (labels). Then split into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f71370b-2817-4ea0-9fd4-a8edbedfee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle frisbee so there is no ordering by plus_minus (there is in the original)\n",
    "frisbee = frisbee.sample(frac = 1)\n",
    "\n",
    "# now, assign the features and labels to X and y\n",
    "\n",
    "# X is frisbee with all the features and not the labels\n",
    "X = frisbee.drop(['is_positive', 'plus_minus'], axis = 1)\n",
    "\n",
    "# y is frisbee but just the label is_positive\n",
    "y = frisbee['is_positive']\n",
    "\n",
    "# pulled straight from exercise, assign the training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size].to_numpy()\n",
    "X_test = X[training_size:].to_numpy()\n",
    "\n",
    "y_train = y[:training_size].to_numpy()\n",
    "y_test = y[training_size:].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74366a-19cd-4222-b04d-c8f95b91ea0e",
   "metadata": {},
   "source": [
    "#### Pull gradient descent functions from Exercise 10-27, for use in both non-DP gradient descent and later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6addaef3-970a-4853-ae20-8f1fc65bb3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8588588588588588)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Since we are doing classification as we were in the exercise, we use the logistic loss function, or cross-entropy function \n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# define the gradient - the rate of change of loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "# define the average gradient for all training instances\n",
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, x_i, y_i) for x_i, y_i in zip(X, y)]\n",
    "    avg_gradient = np.mean(grads, axis=0)\n",
    "    return avg_gradient\n",
    "\n",
    "# function to perform iterations of gradient descent\n",
    "def gradient_descent(iterations):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    for i in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "    return theta\n",
    "\n",
    "# predicts label based on example xi\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "# define accuracy\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "accuracy(gradient_descent(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cf89f-df73-45d3-b4eb-2b9211a18530",
   "metadata": {},
   "source": [
    "#### Implement noisy gradient descent for zero concentrated DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca69f31b-303c-4d19-9482-3df2609819fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8798798798798799\n"
     ]
    }
   ],
   "source": [
    "# helper functions:\n",
    "# L2 clipping of vectors, used for zCDP  gradient descent\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "# L2 clips the gradient for all the training instances\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "    return np.sum(gradients, axis=0)\n",
    "\n",
    "# function that performs noisy gradient descent\n",
    "def gradient_descent_zCDP(iterations, rho):\n",
    "    # make empty theta\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    # after a few experiments, found that b = 15 was the best.. seems high but the data has high values and i didnt want to normalize\n",
    "    # as finding the max was not dp friendly\n",
    "    b = 15\n",
    "    # divvy up our rho\n",
    "    rho_i = rho / (iterations + 1)\n",
    "    # get count and add noise\n",
    "    noisy_count = gaussian_mech_zCDP(X_train.shape[0], 1, rho_i)\n",
    "    # iterate descent\n",
    "    for i in range(iterations):\n",
    "        # clip the gradient\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        # add noise (rho_i per iteration)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_zCDP_vec(clipped_gradient_sum, b, rho_i))\n",
    "        # normalize according to noisy count\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "epsilon = 1.0\n",
    "delta = 1e-5\n",
    "rho = approximate_rho(epsilon, delta)\n",
    "print(accuracy(gradient_descent_zCDP(100,rho)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf079a-7610-4e2d-8ed6-66cd33359c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3e399-49e5-43f6-90de-7b6770841886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9ad10-011d-4a5f-b49a-72601683bbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a2f3a-ed98-449b-8ff8-1ca004635eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5535b7-9b16-472c-b979-530594429113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b602f8-1162-4633-9bb9-6a2b42d6798a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
